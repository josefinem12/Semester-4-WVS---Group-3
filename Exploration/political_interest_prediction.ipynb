{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Gradient Descent Classification for Predicting Political Interest"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import necessary libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "np.random.seed(42)\n",
       "\n",
       "# Display settings\n",
       "pd.set_option('display.max_columns', None)\n",
       "plt.style.use('seaborn-whitegrid')\n",
       "%matplotlib inline"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Loading and Exploring the Dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(\"Loading the dataset... (this may take some time)\")\n",
       "file_path = \"WVS_Time_Series_1981-2022_csv_v5_0.csv\"\n",
       "\n",
       "# Using chunksize to handle large file\n",
       "chunks = pd.read_csv(file_path, chunksize=10000)\n",
       "\n",
       "# Extract only the columns we need to save memory\n",
       "columns_of_interest = ['E023', 'A006', 'A001', 'X025R', 'X003']\n",
       "df_list = []\n",
       "\n",
       "for chunk in chunks:\n",
       "    # Keep only columns we're interested in\n",
       "    df_list.append(chunk[columns_of_interest])\n",
       "\n",
       "# Combine all chunks\n",
       "df = pd.concat(df_list, ignore_index=True)\n",
       "print(f\"Dataset loaded with shape: {df.shape}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Display basic information\n",
       "print(\"\\nBasic information about the dataset:\")\n",
       "print(df.info())\n",
       "\n",
       "# Check unique values in each column\n",
       "print(\"\\nUnique values in each column:\")\n",
       "for col in df.columns:\n",
       "    print(f\"{col}: {df[col].unique()}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Data Cleaning and Preprocessing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(\"\\nCleaning the dataset...\")\n",
       "\n",
       "# Define a function to clean the data by removing invalid codes\n",
       "def clean_column(series):\n",
       "    # Replace invalid codes with NaN\n",
       "    return series.replace([-1, -2, -4, -5], np.nan)\n",
       "\n",
       "# Apply cleaning to all columns\n",
       "for col in df.columns:\n",
       "    df[col] = clean_column(df[col])\n",
       "\n",
       "# Drop rows with missing values\n",
       "df_cleaned = df.dropna()\n",
       "print(f\"Shape after cleaning: {df_cleaned.shape}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Feature engineering\n",
       "print(\"\\nPreparing features and target...\")\n",
       "\n",
       "# For political interest (E023), we'll convert it to binary:\n",
       "# 1 & 2 (Very/Rather important) -> 1 (High interest)\n",
       "# 3 & 4 (Not very/Not at all important) -> 0 (Low interest)\n",
       "df_cleaned['politics_binary'] = df_cleaned['E023'].map(lambda x: 1 if x <= 2 else 0)\n",
       "\n",
       "# Create feature matrix X and target vector y\n",
       "X = df_cleaned[['A006', 'A001', 'X025R', 'X003']]\n",
       "y = df_cleaned['politics_binary']\n",
       "\n",
       "# Scale features\n",
       "scaler = StandardScaler()\n",
       "X_scaled = scaler.fit_transform(X)\n",
       "\n",
       "# Split the data\n",
       "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
       "\n",
       "print(f\"Training data shape: {X_train.shape}\")\n",
       "print(f\"Testing data shape: {X_test.shape}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Data Visualization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Visualize the distribution of features and target\n",
       "plt.figure(figsize=(15, 10))\n",
       "\n",
       "plt.subplot(2, 3, 1)\n",
       "sns.histplot(df_cleaned['E023'], bins=4, kde=True)\n",
       "plt.title('Distribution of Political Interest (E023)')\n",
       "plt.xlabel('Political Interest Level')\n",
       "\n",
       "plt.subplot(2, 3, 2)\n",
       "sns.histplot(df_cleaned['politics_binary'], bins=2, kde=True)\n",
       "plt.title('Binary Political Interest')\n",
       "plt.xlabel('Low (0) vs High (1) Interest')\n",
       "\n",
       "plt.subplot(2, 3, 3)\n",
       "sns.histplot(df_cleaned['A006'], bins=4, kde=True)\n",
       "plt.title('Importance of Religion (A006)')\n",
       "plt.xlabel('Importance Level')\n",
       "\n",
       "plt.subplot(2, 3, 4)\n",
       "sns.histplot(df_cleaned['A001'], bins=4, kde=True)\n",
       "plt.title('Importance of Family (A001)')\n",
       "plt.xlabel('Importance Level')\n",
       "\n",
       "plt.subplot(2, 3, 5)\n",
       "sns.histplot(df_cleaned['X025R'], kde=True)\n",
       "plt.title('Education Level (X025R)')\n",
       "plt.xlabel('Education Level')\n",
       "\n",
       "plt.subplot(2, 3, 6)\n",
       "sns.histplot(df_cleaned['X003'], kde=True)\n",
       "plt.title('Age (X003)')\n",
       "plt.xlabel('Age')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.savefig('feature_distributions.png')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Implementing Gradient Descent for Logistic Regression"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def sigmoid(z):\n",
       "    \"\"\"Sigmoid activation function\"\"\"\n",
       "    return 1 / (1 + np.exp(-z))\n",
       "\n",
       "def compute_cost(X, y, theta, lambda_param=0):\n",
       "    \"\"\"\n",
       "    Compute cost function for logistic regression\n",
       "    \n",
       "    Args:\n",
       "        X: Features matrix\n",
       "        y: Target vector\n",
       "        theta: Parameters vector\n",
       "        lambda_param: Regularization parameter\n",
       "    \n",
       "    Returns:\n",
       "        J: Cost value\n",
       "    \"\"\"\n",
       "    m = len(y)\n",
       "    h = sigmoid(X @ theta)\n",
       "    \n",
       "    # Handle potential numerical issues\n",
       "    epsilon = 1e-15\n",
       "    h = np.clip(h, epsilon, 1 - epsilon)\n",
       "    \n",
       "    # Cross-entropy loss\n",
       "    J = -1/m * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h))\n",
       "    \n",
       "    # Add regularization (excluding the bias term)\n",
       "    if lambda_param > 0:\n",
       "        J += (lambda_param / (2 * m)) * np.sum(np.square(theta[1:]))\n",
       "    \n",
       "    return J\n",
       "\n",
       "def compute_gradient(X, y, theta, lambda_param=0):\n",
       "    \"\"\"\n",
       "    Compute the gradient for logistic regression\n",
       "    \n",
       "    Args:\n",
       "        X: Features matrix\n",
       "        y: Target vector\n",
       "        theta: Parameters vector\n",
       "        lambda_param: Regularization parameter\n",
       "    \n",
       "    Returns:\n",
       "        grad: Gradient vector\n",
       "    \"\"\"\n",
       "    m = len(y)\n",
       "    h = sigmoid(X @ theta)\n",
       "    \n",
       "    # Basic gradient\n",
       "    grad = 1/m * (X.T @ (h - y))\n",
       "    \n",
       "    # Add regularization (excluding the bias term)\n",
       "    if lambda_param > 0:\n",
       "        grad[1:] += (lambda_param / m) * theta[1:]\n",
       "    \n",
       "    return grad\n",
       "\n",
       "def gradient_descent(X, y, theta, alpha, num_iters, lambda_param=0):\n",
       "    \"\"\"\n",
       "    Perform gradient descent to optimize theta\n",
       "    \n",
       "    Args:\n",
       "        X: Features matrix\n",
       "        y: Target vector\n",
       "        theta: Initial parameters\n",
       "        alpha: Learning rate\n",
       "        num_iters: Number of iterations\n",
       "        lambda_param: Regularization parameter\n",
       "    \n",
       "    Returns:\n",
       "        theta: Optimized parameters\n",
       "        J_history: Cost history\n",
       "    \"\"\"\n",
       "    m = len(y)\n",
       "    J_history = []\n",
       "    \n",
       "    for i in range(num_iters):\n",
       "        # Compute gradient\n",
       "        grad = compute_gradient(X, y, theta, lambda_param)\n",
       "        \n",
       "        # Update parameters\n",
       "        theta = theta - alpha * grad\n",
       "        \n",
       "        # Compute and store cost\n",
       "        cost = compute_cost(X, y, theta, lambda_param)\n",
       "        J_history.append(cost)\n",
       "        \n",
       "        # Print progress\n",
       "        if i % 100 == 0:\n",
       "            print(f\"Iteration {i}: Cost = {cost}\")\n",
       "    \n",
       "    return theta, J_history"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Model Training"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Add a column of ones to X for the bias term\n",
       "X_train_with_bias = np.c_[np.ones(X_train.shape[0]), X_train]\n",
       "X_test_with_bias = np.c_[np.ones(X_test.shape[0]), X_test]\n",
       "\n",
       "# Initialize parameters\n",
       "initial_theta = np.zeros(X_train_with_bias.shape[1])\n",
       "\n",
       "# Set hyperparameters\n",
       "alpha = 0.01  # Learning rate\n",
       "num_iters = 1000  # Number of iterations\n",
       "lambda_param = 0.1  # Regularization parameter\n",
       "\n",
       "print(\"\\nTraining logistic regression with gradient descent...\")\n",
       "# Train model\n",
       "theta, J_history = gradient_descent(X_train_with_bias, y_train, initial_theta, alpha, num_iters, lambda_param)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot cost history\n",
       "plt.figure(figsize=(10, 6))\n",
       "plt.plot(J_history)\n",
       "plt.xlabel('Iteration')\n",
       "plt.ylabel('Cost J')\n",
       "plt.title('Cost Function History')\n",
       "plt.grid(True)\n",
       "plt.savefig('cost_history.png')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Model Evaluation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Make predictions\n",
       "y_train_pred_prob = sigmoid(X_train_with_bias @ theta)\n",
       "y_train_pred = (y_train_pred_prob >= 0.5).astype(int)\n",
       "\n",
       "y_test_pred_prob = sigmoid(X_test_with_bias @ theta)\n",
       "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
       "\n",
       "# Evaluate the model\n",
       "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
       "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
       "\n",
       "print(\"\\nModel evaluation:\")\n",
       "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
       "print(f\"Testing accuracy: {test_accuracy:.4f}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Confusion matrix\n",
       "plt.figure(figsize=(8, 6))\n",
       "cm = confusion_matrix(y_test, y_test_pred)\n",
       "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
       "plt.xlabel('Predicted')\n",
       "plt.ylabel('Actual')\n",
       "plt.title('Confusion Matrix')\n",
       "plt.savefig('confusion_matrix.png')\n",
       "plt.show()\n",
       "\n",
       "# Classification report\n",
       "print(\"\\nClassification Report:\")\n",
       "print(classification_report(y_test, y_test_pred))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Feature Importance Analysis"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Feature importance analysis\n",
       "feature_names = ['Bias', 'Religion Importance', 'Family Importance', 'Education Level', 'Age']\n",
       "coef_df = pd.DataFrame({\n",
       "    'Feature': feature_names,\n",
       "    'Coefficient': theta\n",
       "})\n",
       "\n",
       "# Sort by absolute coefficient value\n",
       "coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
       "coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
       "\n",
       "plt.figure(figsize=(10, 6))\n",
       "sns.barplot(x='Coefficient', y='Feature', data=coef_df)\n",
       "plt.title('Feature Importance for Predicting Political Interest')\n",
       "plt.axvline(x=0, color='k', linestyle='--')\n",
       "plt.tight_layout()\n",
       "plt.savefig('feature_importance.png')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Feature Effects Visualization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Analyze model predictions by feature values\n",
       "plt.figure(figsize=(15, 10))\n",
       "\n",
       "# Plot probability of high political interest by age\n",
       "plt.subplot(2, 2, 1)\n",
       "# Create a 2D array with bias=1 and all features at their means except age\n",
       "X_age = np.zeros((100, 5))\n",
       "X_age[:, 0] = 1  # Bias\n",
       "# Set other features to mean values\n",
       "mean_values = np.mean(X_train, axis=0)\n",
       "for i in range(3):  # For features other than age\n",
       "    X_age[:, i+1] = mean_values[i]\n",
       "# Vary age from min to max\n",
       "age_range = np.linspace(np.min(X_train[:, 3]), np.max(X_train[:, 3]), 100)\n",
       "X_age[:, 4] = age_range\n",
       "probs_age = sigmoid(X_age @ theta)\n",
       "\n",
       "plt.plot(age_range, probs_age)\n",
       "plt.xlabel('Age (Normalized)')\n",
       "plt.ylabel('Probability of High Political Interest')\n",
       "plt.title('Effect of Age on Political Interest')\n",
       "plt.grid(True)\n",
       "\n",
       "# Plot probability of high political interest by education\n",
       "plt.subplot(2, 2, 2)\n",
       "X_edu = np.zeros((100, 5))\n",
       "X_edu[:, 0] = 1  # Bias\n",
       "for i in [0, 1, 3]:  # For features other than education\n",
       "    idx = i if i < 2 else i-1\n",
       "    X_edu[:, i+1] = mean_values[idx]\n",
       "# Vary education from min to max\n",
       "edu_range = np.linspace(np.min(X_train[:, 2]), np.max(X_train[:, 2]), 100)\n",
       "X_edu[:, 3] = edu_range\n",
       "probs_edu = sigmoid(X_edu @ theta)\n",
       "\n",
       "plt.plot(edu_range, probs_edu)\n",
       "plt.xlabel('Education Level (Normalized)')\n",
       "plt.ylabel('Probability of High Political Interest')\n",
       "plt.title('Effect of Education on Political Interest')\n",
       "plt.grid(True)\n",
       "\n",
       "# Plot probability by religion importance\n",
       "plt.subplot(2, 2, 3)\n",
       "X_rel = np.zeros((100, 5))\n",
       "X_rel[:, 0] = 1  # Bias\n",
       "for i in [1, 2, 3]:  # For features other than religion\n",
       "    X_rel[:, i+1] = mean_values[i]\n",
       "# Vary religion from min to max\n",
       "rel_range = np.linspace(np.min(X_train[:, 0]), np.max(X_train[:, 0]), 100)\n",
       "X_rel[:, 1] = rel_range\n",
       "probs_rel = sigmoid(X_rel @ theta)\n",
       "\n",
       "plt.plot(rel_range, probs_rel)\n",
       "plt.xlabel('Religion Importance (Normalized)')\n",
       "plt.ylabel('Probability of High Political Interest')\n",
       "plt.title('Effect of Religion on Political Interest')\n",
       "plt.grid(True)\n",
       "\n",
       "# Plot probability by family importance\n",
       "plt.subplot(2, 2, 4)\n",
       "X_fam = np.zeros((100, 5))\n",
       "X_fam[:, 0] = 1  # Bias\n",
       "X_fam[:, 1] = mean_values[0]  # Religion\n",
       "X_fam[:, 3] = mean_values[2]  # Education\n",
       "X_fam[:, 4] = mean_values[3]  # Age\n",
       "# Vary family from min to max\n",
       "fam_range = np.linspace(np.min(X_train[:, 1]), np.max(X_train[:, 1]), 100)\n",
       "X_fam[:, 2] = fam_range\n",
       "probs_fam = sigmoid(X_fam @ theta)\n",
       "\n",
       "plt.plot(fam_range, probs_fam)\n",
       "plt.xlabel('Family Importance (Normalized)')\n",
       "plt.ylabel('Probability of High Political Interest')\n",
       "plt.title('Effect of Family Importance on Political Interest')\n",
       "plt.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.savefig('feature_effects.png')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Conclusion\n",
       "print(\"\\nConclusion:\")\n",
       "print(\"Our model has been trained to predict political interest based on societal values and demographic factors.\")\n",
       "print(f\"Model achieved {test_accuracy:.2%} accuracy on the test set.\")\n",
       "print(\"The most influential features (in order of importance) are:\")\n",
       "for i, row in coef_df.iterrows():\n",
       "    if row['Feature'] != 'Bias':\n",
       "        effect = \"positive\" if row['Coefficient'] > 0 else \"negative\"\n",
       "        print(f\"- {row['Feature']}: {effect} effect (coefficient = {row['Coefficient']:.4f})\")\n",
       "\n",
       "print(\"\\nThese results suggest how societal values and demographics relate to political interest.\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }